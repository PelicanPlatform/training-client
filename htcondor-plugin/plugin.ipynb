{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144ad7bc-2927-4097-8299-dc6f4898a584",
   "metadata": {},
   "source": [
    "# Pelican HTCondor Plugin\n",
    "\n",
    "The real power of Pelican is the ability to provide data to high throughput computing.\n",
    "To demonstrate this, we'll use HTCondor to do a rudimentary analysis of multiple objects of the [NOAA Global Historical Climatology Network](https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html) dataset.\n",
    "\n",
    "## About the data\n",
    "\n",
    "From the [README](https://docs.opendata.aws/noaa-ghcn-pds/readme.html):\n",
    "\n",
    "> GHCN-Daily is a dataset that contains daily observations over global land areas.\n",
    "> It contains station-based measurements from land-based stations worldwide, about two thirds of which are for precipitation measurements only (Menne et al., 2012).\n",
    "> GHCN-Daily is a composite of climate records from numerous sources that were merged together and subjected to a common suite of quality assurance reviews (Durre et al., 2010).\n",
    "\n",
    "The GHCN data set is available via [Amazon's Open Data](https://aws.amazon.com/opendata/) repository, at [https://noaa-ghcn-pds.s3.amazonaws.com/index.html](https://noaa-ghcn-pds.s3.amazonaws.com/index.html).\n",
    "The Open Data repository is already connected to the OSDF under the namespace `aws-opendata`. \n",
    "With a little digging, we find that the NOAA dataset is accessible via `us-east-1/noaa-ghcn-pds`.\n",
    "Altogether, our starting Pelican URL is `osdf:///aws-opendata/us-east-1/noaa-ghcn-pds`.\n",
    "\n",
    "## Exploring the data\n",
    "\n",
    "For this portion, we'll use the Pelican CLI to explore the data, but in principle you can use the PelicanFS Client to accomplish the same thing.\n",
    "\n",
    "First, download a copy of the index file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54821262-9c1a-415c-9322-4d2bf9f8ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelican object get osdf:///aws-opendata/us-east-1/noaa-ghcn-pds/ghcnd-stations.txt ./ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c0449f-a26a-4ed0-9a58-05efa0191fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       \n",
      "ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    \n",
      "AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\n",
      "AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194\n",
      "AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217\n",
      "AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218\n",
      "AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930\n",
      "AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938\n",
      "AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948\n",
      "AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990\n"
     ]
    }
   ],
   "source": [
    "head ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f939f48-0a2f-497d-8852-08c2ea7f63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelican object get osdf:///aws-opendata/us-east-1/noaa-ghcn-pds/csv/by_station/USW00014837.csv ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c754d4-620c-4de6-8eff-7c36a2b85b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,DATE,ELEMENT,DATA_VALUE,M_FLAG,Q_FLAG,S_FLAG,OBS_TIME\n",
      "USW00014837,19391001,TMAX,194,,,X,\n",
      "USW00014837,19391002,TMAX,211,,,X,\n",
      "USW00014837,19391003,TMAX,233,,,X,\n",
      "USW00014837,19391004,TMAX,272,,,X,\n",
      "USW00014837,19391005,TMAX,211,,,X,\n",
      "USW00014837,19391006,TMAX,250,,,X,\n",
      "USW00014837,19391007,TMAX,294,,,X,\n",
      "USW00014837,19391008,TMAX,261,,,X,\n",
      "USW00014837,19391009,TMAX,239,,,X,\n"
     ]
    }
   ],
   "source": [
    "head USW00014837.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd49a1-2eb4-46e9-8548-38c38c87b334",
   "metadata": {},
   "source": [
    "## Rudimentary climate analysis\n",
    "\n",
    "The included script `example.py` performs a very rudimentary analysis of the station data.\n",
    "The script takes the station ID as the argument and creates a corresponding `.png` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4964da4-3a0d-43c4-b02c-7a17339a73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEMENT          TMIN          TMAX\n",
      "count    31176.000000  31176.000000\n",
      "mean        36.415087     56.619965\n",
      "std         20.041446     22.332279\n",
      "min        -36.940000    -14.080000\n",
      "25%         23.000000     37.040000\n",
      "50%         37.040000     59.000000\n",
      "75%         53.060000     77.000000\n",
      "max         82.940000    122.000000\n",
      "\n",
      "\n",
      "Plotting histograms of observations for 31,176 days, spanning 85.4 years \n",
      "from 1939-10-01 to 2025-02-06, to 'USW00014837.png' .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "./example.py USW00014837"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1b7c6-fec9-49a9-a186-d6a97024e855",
   "metadata": {},
   "source": [
    "Take a look at the new `.png` file!\n",
    "\n",
    "## Scaling out\n",
    "\n",
    "There are a lot of stations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279ce9e4-58fa-42a3-97a8-6334584fd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129657 ghcnd-stations.txt\n"
     ]
    }
   ],
   "source": [
    "wc -l ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c58b37-bbdb-43b4-93ac-2cd9bbaf8534",
   "metadata": {},
   "source": [
    "Suppose you want to do a proper climate analysis, but it takes 1 hour to run per station.\n",
    "\n",
    "If executed serially (one after the next), this would take ~130,000 hours, or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82058d41-3456-41af-a92f-38f58211268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.80 years!\n"
     ]
    }
   ],
   "source": [
    "python3 -c 'print(f\"{(129657 / 24 / 365):.2f} years!\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef0cb0-b3d3-497b-9269-bf37be06ec37",
   "metadata": {},
   "source": [
    "The power of HTCondor and high throughput computing is the ability to place many individual calculations across many computers.\n",
    "Users of the OSPool and similar high throughput computing systems regularly run *thousands* of jobs at time. \n",
    "\n",
    "At a rate of 1,000 stations analyzed per hour, the \"real\" runtime becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af21f62-56ab-494c-8618-5756ba354c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.40 days!\n"
     ]
    }
   ],
   "source": [
    "python3 -c 'print(f\"{(129657 / 1000 / 24):.2f} days!\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0cdc7-634a-4a12-8fb6-fd11b428116c",
   "metadata": {},
   "source": [
    "Finally, since the data is available via the OSDF, you don't have to worry about moving the data around as part of the compute.\n",
    "Just provide the necessary Pelican URLs!\n",
    "\n",
    "## Scaling out with HTCondor and Pelican\n",
    "\n",
    "The included `example.sub` demonstrates how to run the rudimentary climate analysis on 10 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d40deb5-fe56-46a1-a1b4-d77b80161492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container_image = osdf:///ospool/uc-shared/public/OSG-Staff/training/python.sif\n",
      "\n",
      "executable   = example.py\n",
      "arguments    = $(STATION_ID)\n",
      "\n",
      "OSDF_PREFIX  = osdf:///aws-opendata/us-east-1/noaa-ghcn-pds/csv/by_station\n",
      "transfer_input_files = my_functions.py, $(OSDF_PREFIX)/$(STATION_ID).csv\n",
      "\n",
      "should_transfer_files = YES\n",
      "transfer_output_remaps = \"$(STATION_ID).png=results/$(STATION_ID).png\"\n",
      "\n",
      "log          = logs/example.$(Cluster).log\n",
      "output       = logs/$(STATION_ID).out\n",
      "error        = logs/$(STATION_ID).err\n",
      "\n",
      "request_cpus   = 1\n",
      "request_memory = 2GB\n",
      "request_disk   = 2GB\n",
      "\n",
      "queue STATION_ID from station_list.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat example.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fcf1d-b704-439f-837f-fb39dea367b5",
   "metadata": {},
   "source": [
    "The only thing that is needed is the `station_list.txt` file with the list of station IDs for the datasets you want to analyze.\n",
    "\n",
    "You can generate this list with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7497076d-30c5-4c1f-a798-e585e9df785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "./generate_list.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6df343-a5fd-4565-b488-424bc7851e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USW00014837\n",
      "USW00014838\n",
      "USW00014839\n",
      "USW00014840\n",
      "USW00014841\n",
      "USW00014842\n",
      "USW00014843\n",
      "USW00014844\n",
      "USW00014845\n",
      "USW00014846\n"
     ]
    }
   ],
   "source": [
    "cat station_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc6ad7-8f0a-4112-aea3-719f662f0428",
   "metadata": {},
   "source": [
    "## Submitting a list of jobs\n",
    "\n",
    "With the set-up complete, you can submit the list of jobs as usual using HTCondor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24f1e8a-88b9-45e6-aebe-5205184abab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job(s)..........\n",
      "10 job(s) submitted to cluster 1.\n"
     ]
    }
   ],
   "source": [
    "condor_submit example.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463eef5-357a-423d-9c6f-1dc56e8fd9f2",
   "metadata": {},
   "source": [
    "Then monitor the progress of the jobs with `condor_q`, or by running `condor_watch_q` in the terminal console (doesn't work right in the notebook interface..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d254e0e-6c9b-415f-accb-0693b766ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- Schedd: jovyan@jupyter-aowen4-wisc-edu---0bf5e8be : <10.129.164.237:9618?... @ 06/04/25 15:05:52\n",
      "OWNER BATCH_NAME      SUBMITTED   DONE   RUN    IDLE   HOLD  TOTAL JOB_IDS\n",
      "\n",
      "Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended \n",
      "Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condor_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4f648-a10e-484d-b3e4-fd776620050a",
   "metadata": {},
   "source": [
    "Once complete, you should see a bunch of image files in the `results/` directory. \n",
    "\n",
    "You'll be shocked to learn that winters are colder than summers (at least in Wisconsin)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:base] *",
   "language": "bash",
   "name": "conda-base-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
